{"title":"Exploratory data analysis and machine learining","markdown":{"yaml":{"title":"Exploratory data analysis and machine learining ","author":"Nikhil Rachagani","date":"2024-01-15","categories":["code","analysis"],"image":"image.jpg"},"headingText":"Install and load required libraries","containsRefs":false,"markdown":"\n\nThe R code is a concise script for exploring and comparing two classification models -- logistic regression and random forest -- on a randomly generated dataset. The code begins by installing and loading necessary libraries, then generates a synthetic dataframe with features (feature1, feature2, feature3) and a binary target variable. After visualizing the data through a scatter plot, it splits the dataset into training and testing sets. Two machine learning models are trained and evaluated on the test set: logistic regression and random forest. The logistic regression model is implemented using the glm function, while the random forest model is built using the randomForest function. The code concludes with a comparison of model accuracies. However, there is a missing reference (accuracy_lasso) and an undeclared lasso model, which could be addressed for a more comprehensive analysis.\n\n```{r}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(glmnet)\nlibrary(xgboost)\nlibrary(pROC)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate random dataframe\nn <- 1000\ndf <- data.frame(\n  feature1 = rnorm(n),\n  feature2 = rnorm(n),\n  feature3 = rnorm(n),\n  target = rbinom(n, 1, 0.5)\n)\n\n# EDA\nsummary(df)\nstr(df)\ncor(df)\n\n# Visualization\nggplot(df, aes(x = feature1, y = feature2, color = as.factor(target))) +\n  geom_point() +\n  ggtitle(\"Scatter plot of Feature1 and Feature2\") +\n  theme_minimal()\n# Pairwise scatter plots with color by target\nplot_pairs <- ggplot(df, aes(color = as.factor(target))) +\n  geom_point(aes(x = feature1, y = feature2)) +\n  geom_point(aes(x = feature2, y = feature3)) +\n  geom_point(aes(x = feature1, y = feature3)) +\n  ggtitle(\"Pairwise Scatter Plots with Color by Target\") +\n  theme_minimal()\n\n# Display the plot\nprint(plot_pairs)\n# Distribution of each feature by target\nplot_dist <- ggplot(df, aes(x = feature1, fill = as.factor(target))) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"Distribution of Feature1 by Target\") +\n  theme_minimal()\n\n# Display the plot\nprint(plot_dist)\n# Similar plots for other features (feature2, feature3)\nplot_dist_feature2 <- ggplot(df, aes(x = feature2, fill = as.factor(target))) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"Distribution of Feature2 by Target\") +\n  theme_minimal()\n\nplot_dist_feature3 <- ggplot(df, aes(x = feature3, fill = as.factor(target))) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"Distribution of Feature3 by Target\") +\n  theme_minimal()\n\n# Display the plots\nprint(plot_dist_feature2)\nprint(plot_dist_feature3)\n# Boxplots for each feature by target\nplot_boxplot <- ggplot(df, aes(x = as.factor(target), y = feature1, fill = as.factor(target))) +\n  geom_boxplot() +\n  ggtitle(\"Boxplot of Feature1 by Target\") +\n  theme_minimal()\n\n# Display the plot\nprint(plot_boxplot)\n# Similar plots for other features (feature2, feature3)\nplot_boxplot_feature2 <- ggplot(df, aes(x = as.factor(target), y = feature2, fill = as.factor(target))) +\n  geom_boxplot() +\n  ggtitle(\"Boxplot of Feature2 by Target\") +\n  theme_minimal()\n\nplot_boxplot_feature3 <- ggplot(df, aes(x = as.factor(target), y = feature3, fill = as.factor(target))) +\n  geom_boxplot() +\n  ggtitle(\"Boxplot of Feature3 by Target\") +\n  theme_minimal()\n\n# Display the plots\nprint(plot_boxplot_feature2)\nprint(plot_boxplot_feature3)\n\n```\n\n```{r}\n# Split data into training and testing sets\nset.seed(456)\nsplitIndex <- createDataPartition(df$target, p = 0.7, list = FALSE)\ntrain_data <- df[splitIndex, ]\ntest_data <- df[-splitIndex, ]\n\n# Machine Learning Models\n# Model 1: Logistic Regression\nmodel_logreg <- glm(target ~ ., data = train_data, family = \"binomial\")\npred_logreg <- predict(model_logreg, newdata = test_data, type = \"response\")\n\n# Visualize the logistic regression model\nsummary(model_logreg)\n\n# Print the confusion matrix\nconf_matrix <- confusionMatrix(table(Actual = test_data$target, Predicted = round(pred_logreg)))\nprint(conf_matrix)\n\n```\n\n```{r}\n# Model 2: Random Forest\nmodel_rf <- randomForest(target ~ ., data = train_data)\npred_rf <- predict(model_rf, newdata = test_data, type = \"response\")\n\n# Convert predictions to binary factor\npred_rf_binary <- ifelse(pred_rf > 0.5, 1, 0)\n\n# Evaluate and print results for Model 2\nconfusion_matrix_rf <- confusionMatrix(factor(pred_rf_binary), factor(test_data$target))\ncat(\"Model 2 (Random Forest) Results:\\n\")\nprint(confusion_matrix_rf)\n```\n\n```{r}\n# Model Comparison\n# Assuming binary classification, you can compare the accuracy of the models\naccuracy_logreg <- confusionMatrix(factor(round(pred_logreg)), factor(test_data$target))$overall[\"Accuracy\"]\naccuracy_rf <- confusion_matrix_rf$overall[\"Accuracy\"]\n\n# Print the comparison results\ncat(\"\\nModel Comparison:\\n\")\ncat(\"Logistic Regression Accuracy:\", accuracy_logreg, \"\\n\")\ncat(\"Random Forest Accuracy:\", accuracy_rf, \"\\n\")\n```\n\nThe logistic regression model outperformed both the random forest and lasso regression models, achieving an accuracy of 0.5833. In comparison, the random forest model had an accuracy of 0.5333, and the lasso regression model had the lowest accuracy of 0.5233. Therefore, the logistic regression model appears to be the better-performing model among the three based on the given accuracy metrics. However, it's essential to consider additional evaluation metrics and potential overfitting or underfitting issues to make a more comprehensive assessment of model performance.\n","srcMarkdownNoYaml":"\n\nThe R code is a concise script for exploring and comparing two classification models -- logistic regression and random forest -- on a randomly generated dataset. The code begins by installing and loading necessary libraries, then generates a synthetic dataframe with features (feature1, feature2, feature3) and a binary target variable. After visualizing the data through a scatter plot, it splits the dataset into training and testing sets. Two machine learning models are trained and evaluated on the test set: logistic regression and random forest. The logistic regression model is implemented using the glm function, while the random forest model is built using the randomForest function. The code concludes with a comparison of model accuracies. However, there is a missing reference (accuracy_lasso) and an undeclared lasso model, which could be addressed for a more comprehensive analysis.\n\n```{r}\n# Install and load required libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(glmnet)\nlibrary(xgboost)\nlibrary(pROC)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate random dataframe\nn <- 1000\ndf <- data.frame(\n  feature1 = rnorm(n),\n  feature2 = rnorm(n),\n  feature3 = rnorm(n),\n  target = rbinom(n, 1, 0.5)\n)\n\n# EDA\nsummary(df)\nstr(df)\ncor(df)\n\n# Visualization\nggplot(df, aes(x = feature1, y = feature2, color = as.factor(target))) +\n  geom_point() +\n  ggtitle(\"Scatter plot of Feature1 and Feature2\") +\n  theme_minimal()\n# Pairwise scatter plots with color by target\nplot_pairs <- ggplot(df, aes(color = as.factor(target))) +\n  geom_point(aes(x = feature1, y = feature2)) +\n  geom_point(aes(x = feature2, y = feature3)) +\n  geom_point(aes(x = feature1, y = feature3)) +\n  ggtitle(\"Pairwise Scatter Plots with Color by Target\") +\n  theme_minimal()\n\n# Display the plot\nprint(plot_pairs)\n# Distribution of each feature by target\nplot_dist <- ggplot(df, aes(x = feature1, fill = as.factor(target))) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"Distribution of Feature1 by Target\") +\n  theme_minimal()\n\n# Display the plot\nprint(plot_dist)\n# Similar plots for other features (feature2, feature3)\nplot_dist_feature2 <- ggplot(df, aes(x = feature2, fill = as.factor(target))) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"Distribution of Feature2 by Target\") +\n  theme_minimal()\n\nplot_dist_feature3 <- ggplot(df, aes(x = feature3, fill = as.factor(target))) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"Distribution of Feature3 by Target\") +\n  theme_minimal()\n\n# Display the plots\nprint(plot_dist_feature2)\nprint(plot_dist_feature3)\n# Boxplots for each feature by target\nplot_boxplot <- ggplot(df, aes(x = as.factor(target), y = feature1, fill = as.factor(target))) +\n  geom_boxplot() +\n  ggtitle(\"Boxplot of Feature1 by Target\") +\n  theme_minimal()\n\n# Display the plot\nprint(plot_boxplot)\n# Similar plots for other features (feature2, feature3)\nplot_boxplot_feature2 <- ggplot(df, aes(x = as.factor(target), y = feature2, fill = as.factor(target))) +\n  geom_boxplot() +\n  ggtitle(\"Boxplot of Feature2 by Target\") +\n  theme_minimal()\n\nplot_boxplot_feature3 <- ggplot(df, aes(x = as.factor(target), y = feature3, fill = as.factor(target))) +\n  geom_boxplot() +\n  ggtitle(\"Boxplot of Feature3 by Target\") +\n  theme_minimal()\n\n# Display the plots\nprint(plot_boxplot_feature2)\nprint(plot_boxplot_feature3)\n\n```\n\n```{r}\n# Split data into training and testing sets\nset.seed(456)\nsplitIndex <- createDataPartition(df$target, p = 0.7, list = FALSE)\ntrain_data <- df[splitIndex, ]\ntest_data <- df[-splitIndex, ]\n\n# Machine Learning Models\n# Model 1: Logistic Regression\nmodel_logreg <- glm(target ~ ., data = train_data, family = \"binomial\")\npred_logreg <- predict(model_logreg, newdata = test_data, type = \"response\")\n\n# Visualize the logistic regression model\nsummary(model_logreg)\n\n# Print the confusion matrix\nconf_matrix <- confusionMatrix(table(Actual = test_data$target, Predicted = round(pred_logreg)))\nprint(conf_matrix)\n\n```\n\n```{r}\n# Model 2: Random Forest\nmodel_rf <- randomForest(target ~ ., data = train_data)\npred_rf <- predict(model_rf, newdata = test_data, type = \"response\")\n\n# Convert predictions to binary factor\npred_rf_binary <- ifelse(pred_rf > 0.5, 1, 0)\n\n# Evaluate and print results for Model 2\nconfusion_matrix_rf <- confusionMatrix(factor(pred_rf_binary), factor(test_data$target))\ncat(\"Model 2 (Random Forest) Results:\\n\")\nprint(confusion_matrix_rf)\n```\n\n```{r}\n# Model Comparison\n# Assuming binary classification, you can compare the accuracy of the models\naccuracy_logreg <- confusionMatrix(factor(round(pred_logreg)), factor(test_data$target))$overall[\"Accuracy\"]\naccuracy_rf <- confusion_matrix_rf$overall[\"Accuracy\"]\n\n# Print the comparison results\ncat(\"\\nModel Comparison:\\n\")\ncat(\"Logistic Regression Accuracy:\", accuracy_logreg, \"\\n\")\ncat(\"Random Forest Accuracy:\", accuracy_rf, \"\\n\")\n```\n\nThe logistic regression model outperformed both the random forest and lasso regression models, achieving an accuracy of 0.5833. In comparison, the random forest model had an accuracy of 0.5333, and the lasso regression model had the lowest accuracy of 0.5233. Therefore, the logistic regression model appears to be the better-performing model among the three based on the given accuracy metrics. However, it's essential to consider additional evaluation metrics and potential overfitting or underfitting issues to make a more comprehensive assessment of model performance.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","editor":"visual","theme":"journal","title-block-banner":true,"title":"Exploratory data analysis and machine learining ","author":"Nikhil Rachagani","date":"2024-01-15","categories":["code","analysis"],"image":"image.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}