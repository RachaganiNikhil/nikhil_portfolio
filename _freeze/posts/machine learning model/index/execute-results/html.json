{
  "hash": "6a02d9d2bbde0b12d89c1333037a9ae1",
  "result": {
    "markdown": "---\ntitle: \"Exploratory data analysis and machine learining \"\nauthor: \"Nikhil Rachagani\"\ndate: \"2024-01-15\"\ncategories: [code, analysis]\nimage: \"image.jpg\"\n---\n\n\nThe R code is a concise script for exploring and comparing two classification models -- logistic regression and random forest -- on a randomly generated dataset. The code begins by installing and loading necessary libraries, then generates a synthetic dataframe with features (feature1, feature2, feature3) and a binary target variable. After visualizing the data through a scatter plot, it splits the dataset into training and testing sets. Two machine learning models are trained and evaluated on the test set: logistic regression and random forest. The logistic regression model is implemented using the glm function, while the random forest model is built using the randomForest function. The code concludes with a comparison of model accuracies. However, there is a missing reference (accuracy_lasso) and an undeclared lasso model, which could be addressed for a more comprehensive analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load required libraries\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n```\n:::\n\n```{.r .cell-code}\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrandomForest 4.7-1.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'randomForest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n:::\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'Matrix'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded glmnet 4.1-8\n```\n:::\n\n```{.r .cell-code}\nlibrary(xgboost)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'xgboost'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    slice\n```\n:::\n\n```{.r .cell-code}\nlibrary(pROC)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nType 'citation(\"pROC\")' for a citation.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'pROC'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n```\n:::\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate random dataframe\nn <- 1000\ndf <- data.frame(\n  feature1 = rnorm(n),\n  feature2 = rnorm(n),\n  feature3 = rnorm(n),\n  target = rbinom(n, 1, 0.5)\n)\n\n# EDA\nsummary(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    feature1           feature2           feature3            target     \n Min.   :-2.80978   Min.   :-3.04786   Min.   :-2.84855   Min.   :0.000  \n 1st Qu.:-0.62832   1st Qu.:-0.65322   1st Qu.:-0.65619   1st Qu.:0.000  \n Median : 0.00921   Median : 0.05485   Median :-0.05057   Median :0.000  \n Mean   : 0.01613   Mean   : 0.04247   Mean   :-0.02011   Mean   :0.486  \n 3rd Qu.: 0.66460   3rd Qu.: 0.75345   3rd Qu.: 0.64258   3rd Qu.:1.000  \n Max.   : 3.24104   Max.   : 3.39037   Max.   : 3.42110   Max.   :1.000  \n```\n:::\n\n```{.r .cell-code}\nstr(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t1000 obs. of  4 variables:\n $ feature1: num  -0.5605 -0.2302 1.5587 0.0705 0.1293 ...\n $ feature2: num  -0.996 -1.04 -0.018 -0.132 -2.549 ...\n $ feature3: num  -0.512 0.237 -0.542 1.219 0.174 ...\n $ target  : int  0 0 0 1 0 1 0 1 1 0 ...\n```\n:::\n\n```{.r .cell-code}\ncor(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            feature1    feature2    feature3      target\nfeature1  1.00000000  0.08647944 -0.01932954 -0.07269625\nfeature2  0.08647944  1.00000000  0.02650333 -0.03986642\nfeature3 -0.01932954  0.02650333  1.00000000 -0.04993487\ntarget   -0.07269625 -0.03986642 -0.04993487  1.00000000\n```\n:::\n\n```{.r .cell-code}\n# Visualization\nggplot(df, aes(x = feature1, y = feature2, color = as.factor(target))) +\n  geom_point() +\n  ggtitle(\"Scatter plot of Feature1 and Feature2\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Pairwise scatter plots with color by target\nplot_pairs <- ggplot(df, aes(color = as.factor(target))) +\n  geom_point(aes(x = feature1, y = feature2)) +\n  geom_point(aes(x = feature2, y = feature3)) +\n  geom_point(aes(x = feature1, y = feature3)) +\n  ggtitle(\"Pairwise Scatter Plots with Color by Target\") +\n  theme_minimal()\n\n# Display the plot\nprint(plot_pairs)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Distribution of each feature by target\nplot_dist <- ggplot(df, aes(x = feature1, fill = as.factor(target))) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"Distribution of Feature1 by Target\") +\n  theme_minimal()\n\n# Display the plot\nprint(plot_dist)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Similar plots for other features (feature2, feature3)\nplot_dist_feature2 <- ggplot(df, aes(x = feature2, fill = as.factor(target))) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"Distribution of Feature2 by Target\") +\n  theme_minimal()\n\nplot_dist_feature3 <- ggplot(df, aes(x = feature3, fill = as.factor(target))) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"Distribution of Feature3 by Target\") +\n  theme_minimal()\n\n# Display the plots\nprint(plot_dist_feature2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-4.png){width=672}\n:::\n\n```{.r .cell-code}\nprint(plot_dist_feature3)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-5.png){width=672}\n:::\n\n```{.r .cell-code}\n# Boxplots for each feature by target\nplot_boxplot <- ggplot(df, aes(x = as.factor(target), y = feature1, fill = as.factor(target))) +\n  geom_boxplot() +\n  ggtitle(\"Boxplot of Feature1 by Target\") +\n  theme_minimal()\n\n# Display the plot\nprint(plot_boxplot)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-6.png){width=672}\n:::\n\n```{.r .cell-code}\n# Similar plots for other features (feature2, feature3)\nplot_boxplot_feature2 <- ggplot(df, aes(x = as.factor(target), y = feature2, fill = as.factor(target))) +\n  geom_boxplot() +\n  ggtitle(\"Boxplot of Feature2 by Target\") +\n  theme_minimal()\n\nplot_boxplot_feature3 <- ggplot(df, aes(x = as.factor(target), y = feature3, fill = as.factor(target))) +\n  geom_boxplot() +\n  ggtitle(\"Boxplot of Feature3 by Target\") +\n  theme_minimal()\n\n# Display the plots\nprint(plot_boxplot_feature2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-7.png){width=672}\n:::\n\n```{.r .cell-code}\nprint(plot_boxplot_feature3)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-8.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split data into training and testing sets\nset.seed(456)\nsplitIndex <- createDataPartition(df$target, p = 0.7, list = FALSE)\ntrain_data <- df[splitIndex, ]\ntest_data <- df[-splitIndex, ]\n\n# Machine Learning Models\n# Model 1: Logistic Regression\nmodel_logreg <- glm(target ~ ., data = train_data, family = \"binomial\")\npred_logreg <- predict(model_logreg, newdata = test_data, type = \"response\")\n\n# Visualize the logistic regression model\nsummary(model_logreg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = target ~ ., family = \"binomial\", data = train_data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept) -0.036874   0.075828  -0.486    0.627\nfeature1    -0.102390   0.076979  -1.330    0.183\nfeature2    -0.007029   0.076266  -0.092    0.927\nfeature3    -0.058923   0.076075  -0.775    0.439\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 970.13  on 699  degrees of freedom\nResidual deviance: 967.70  on 696  degrees of freedom\nAIC: 975.7\n\nNumber of Fisher Scoring iterations: 3\n```\n:::\n\n```{.r .cell-code}\n# Print the confusion matrix\nconf_matrix <- confusionMatrix(table(Actual = test_data$target, Predicted = round(pred_logreg)))\nprint(conf_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n      Predicted\nActual   0   1\n     0 105  52\n     1  73  70\n                                          \n               Accuracy : 0.5833          \n                 95% CI : (0.5253, 0.6397)\n    No Information Rate : 0.5933          \n    P-Value [Acc > NIR] : 0.66068         \n                                          \n                  Kappa : 0.1593          \n                                          \n Mcnemar's Test P-Value : 0.07364         \n                                          \n            Sensitivity : 0.5899          \n            Specificity : 0.5738          \n         Pos Pred Value : 0.6688          \n         Neg Pred Value : 0.4895          \n             Prevalence : 0.5933          \n         Detection Rate : 0.3500          \n   Detection Prevalence : 0.5233          \n      Balanced Accuracy : 0.5818          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model 2: Random Forest\nmodel_rf <- randomForest(target ~ ., data = train_data)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n```\n:::\n\n```{.r .cell-code}\npred_rf <- predict(model_rf, newdata = test_data, type = \"response\")\n\n# Convert predictions to binary factor\npred_rf_binary <- ifelse(pred_rf > 0.5, 1, 0)\n\n# Evaluate and print results for Model 2\nconfusion_matrix_rf <- confusionMatrix(factor(pred_rf_binary), factor(test_data$target))\ncat(\"Model 2 (Random Forest) Results:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel 2 (Random Forest) Results:\n```\n:::\n\n```{.r .cell-code}\nprint(confusion_matrix_rf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 95 78\n         1 62 65\n                                          \n               Accuracy : 0.5333          \n                 95% CI : (0.4751, 0.5909)\n    No Information Rate : 0.5233          \n    P-Value [Acc > NIR] : 0.3866          \n                                          \n                  Kappa : 0.0599          \n                                          \n Mcnemar's Test P-Value : 0.2049          \n                                          \n            Sensitivity : 0.6051          \n            Specificity : 0.4545          \n         Pos Pred Value : 0.5491          \n         Neg Pred Value : 0.5118          \n             Prevalence : 0.5233          \n         Detection Rate : 0.3167          \n   Detection Prevalence : 0.5767          \n      Balanced Accuracy : 0.5298          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model Comparison\n# Assuming binary classification, you can compare the accuracy of the models\naccuracy_logreg <- confusionMatrix(factor(round(pred_logreg)), factor(test_data$target))$overall[\"Accuracy\"]\naccuracy_rf <- confusion_matrix_rf$overall[\"Accuracy\"]\n\n# Print the comparison results\ncat(\"\\nModel Comparison:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nModel Comparison:\n```\n:::\n\n```{.r .cell-code}\ncat(\"Logistic Regression Accuracy:\", accuracy_logreg, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Accuracy: 0.5833333 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Random Forest Accuracy:\", accuracy_rf, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Accuracy: 0.5333333 \n```\n:::\n:::\n\n\nThe logistic regression model outperformed both the random forest and lasso regression models, achieving an accuracy of 0.5833. In comparison, the random forest model had an accuracy of 0.5333, and the lasso regression model had the lowest accuracy of 0.5233. Therefore, the logistic regression model appears to be the better-performing model among the three based on the given accuracy metrics. However, it's essential to consider additional evaluation metrics and potential overfitting or underfitting issues to make a more comprehensive assessment of model performance.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}